
@article{abhishekWeatherForecastingModel2012,
  title = {Weather {{Forecasting Model}} Using {{Artificial Neural Network}}},
  author = {Abhishek, Kumar and Singh, M.P. and Ghosh, Saswata and Anand, Abhishek},
  year = {2012},
  volume = {4},
  pages = {311--318},
  issn = {22120173},
  doi = {10.1016/j.protcy.2012.05.047},
  abstract = {Weather forecasting has become an important field of research in the last few decades. In most of the cases the researcher had attempted to establish a linear relationship between the input weather data and the corresponding target data. But with the discovery of nonlinearity in the nature of weather data, the focus has shifted towards the nonlinear prediction of the weather data. Although, there are many literatures in nonlinear statistics for the weather forecasting, most of them required that the nonlinear model be specified before the estimation is done. But since the weather data is nonlinear and follows a very irregular trend, Artificial Neural Network (ANN) has evolved out to be a better technique to bring out the structural relationship between the various entities. The paper examines the applicability of ANN approach by developing effective and reliable nonlinear predictive models for weather analysis also compare and evaluate the performance of the developed models using different transfer functions, hidden layers and neurons to forecast maximum, temperature for 365 days of the year.},
  file = {/Users/isalykkehansen/Zotero/storage/V289Z6N6/Abhishek et al. - 2012 - Weather Forecasting Model using Artificial Neural .pdf},
  journal = {Procedia Technology},
  language = {en}
}

@inproceedings{bansalBlurImageDetection2016,
  title = {Blur Image Detection Using {{Laplacian}} Operator and {{Open}}-{{CV}}},
  booktitle = {2016 {{International Conference System Modeling Advancement}} in {{Research Trends}} ({{SMART}})},
  author = {Bansal, Raghav and Raj, Gaurav and Choudhury, Tanupriya},
  year = {2016},
  month = nov,
  pages = {63--67},
  doi = {10.1109/SYSMART.2016.7894491},
  abstract = {With the increased usage of digital cameras and picture clicking devices, the number of digital images increases rapidly, which in return demand for image quality assessment in terms of blur. Based on the edge type and sharpness analysis using Laplacian operator, an effective representation of blur image detection scheme is proposed in this paper, which can determine that whether the image is blurred or not, and what is the extent of blur through Variance of Laplacian. In this project a simple, reliable and fast algorithm for image noise estimation is presented. Images taken for the testing is assumed to be corrupted by additive zero mean Gaussian noise. To exclude structures or details from contributive to the noise variance estimation, an easy edge detection algorithm inculcating first-order gradients is applied first. Then a Laplacian operator followed by an averaging over the whole image will provide very accurate noise variance estimation. There is only one parameter which is self-determined and adaptive to the image contents. Simulation results show that the proposed algorithm performs well for different types of images over a large range of noise variances. Performance comparisons against alternative approaches are also provided. Results obtain through screenshots of the experiments demonstrate the effectiveness of the proposed scheme.},
  file = {/Users/isalykkehansen/Zotero/storage/KH4EG9HB/7894491.html},
  keywords = {additive zero mean Gaussian noise,blur image detection,Computer vision,Computer Vision,Degradation,digital cameras,digital images,Digital images,edge detection,edge detection algorithm,edge type,Estimation,estimation theory,first-order gradients,Gaussian noise,image contents,image denoising,Image edge detection,image noise estimation,image quality assessment,image representation,image restoration,image testing,Laplace equations,Laplacian operator,Laplacian Operator,Laplacian variance,mathematical operators,noise variance estimation,Open-CV,picture clicking devices,Python,sharpness analysis}
}

@misc{BeginnerGuideImplementing2019a,
  title = {The Beginner's Guide to Implementing {{Yolov3}} in {{TensorFlow}} 2.0},
  year = {2019},
  month = dec,
  abstract = {In this tutorial, I'll be sharing how to implement the YOLOv3 object detector using TensorFlow 2 in the simplest way. Without over-complicating things, ...},
  chapter = {Python Programming},
  journal = {Machine Learning Space},
  language = {en}
}

@misc{brownleeHowPerformObject2019,
  title = {How to {{Perform Object Detection With YOLOv3}} in {{Keras}}},
  author = {Brownlee, Jason},
  year = {2019},
  month = may,
  abstract = {Object detection is a task in computer vision that involves identifying the presence, location, and type of one or more objects in a given photograph. It is a challenging problem that involves building upon methods for object recognition (e.g. where are they), object localization (e.g. what are their extent), and object classification (e.g. what are [\ldots ]},
  file = {/Users/isalykkehansen/Zotero/storage/2SNATYFH/how-to-perform-object-detection-with-yolov3-in-keras.html},
  journal = {Machine Learning Mastery},
  language = {en-US}
}

@misc{C4W3L01ObjectLocalization2017,
  title = {{{C4W3L01 Object Localization}}},
  year = {2017},
  month = nov,
  abstract = {Take the Deep Learning Specialization: http://bit.ly/2IpmuHg
Check out all our courses: https://www.deeplearning.ai
Subscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch

Follow us: 
Twitter: https://twitter.com/deeplearningai\_
Facebook: https://www.facebook.com/deeplearningHQ/
Linkedin: https://www.linkedin.com/company/deep...}
}

@misc{C4W3L08AnchorBoxes,
  title = {{{C4W3L08 Anchor Boxes}}},
  abstract = {Take the Deep Learning Specialization: http://bit.ly/2TtgW58
Check out all our courses: https://www.deeplearning.ai
Subscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch

Follow us: 
Twitter: https://twitter.com/deeplearningai\_
Facebook: https://www.facebook.com/deeplearningHQ/
Linkedin: https://www.linkedin.com/company/deep...}
}

@misc{ClimateChangeEcology,
  title = {Climate {{Change Ecology}} | {{IRSAE}}},
  file = {/Users/isalykkehansen/Zotero/storage/59R6KQEB/4863.html},
  language = {en-US}
}

@misc{danmarksfrieforskningsfondAretsOriginaleIde2019,
  title = {\AA rets Originale Id\'e 2018},
  author = {Danmarks Frie Forskningsfond},
  year = {2019},
  month = mar,
  abstract = {Som den f\o rste modtog seniorforsker Toke H\o ye fra Aarhus Universitet prisen \AA rets originale id\'e 2018. Ved at udvikle en avanceret computerbaseret metode, der involverer machine learning, vil han g\o re det muligt at automatisere kamerasystemer, som skal indsamle imponerende datam\ae ngder til kortl\ae gning af interaktionen mellem best\o vende insekter og blomster i Arktis.

Prisen er oprettet af Danmarks Frie Forskningsfond i 2018.}
}

@article{ekiciBreastCancerDiagnosis2020,
  title = {Breast Cancer Diagnosis Using Thermography and Convolutional Neural Networks},
  author = {Ekici, Sami and Jawzal, Hushang},
  year = {2020},
  month = apr,
  volume = {137},
  pages = {109542},
  issn = {0306-9877},
  doi = {10.1016/j.mehy.2019.109542},
  abstract = {Thermography is an entirely non-invasive and non-contact imaging technique that is widely used in the medicinal field. Since the early detection of cancer is very important, the computer-aided system can increase the rate of diagnosis, cure, and survival of the affected person. Considering the high cost of treatment in addition to the high prevalence of affected persons, early diagnosis is the most important step in reducing the health and social complications of this disease. Currently, mammography is the main method used for screening breast cancer. However, for young woman, mammography is not recommended due to the low contrast that results from the dense breast, and alternative techniques must be considered for this purpose. Breast cancer is the main cause of cancer-related mortality among women. Early detection of cancer-especially breast cancer-will aid the treatment process. Our goal is to develop software for detecting breast cancer automatically that uses image-processing techniques and algorithms to analyze thermal breast images to detect the signs of the disease in these images, allowing the early detection of breast cancer. A new algorithm is proposed for the extraction of the breast characteristic features based on bio-data, image analysis, and image statistics. These features have been extracted from the thermal images captured by a thermal camera, and will be used to classify the breast images as normal or suspected by using convolutional neural networks (CNNs) optimized by Bayes algorithm. By using our proposed algorithm, a 98.95\% of accuracy rate was obtained for the thermal images in the dataset belonging to 140 individuals.},
  file = {/Users/isalykkehansen/Zotero/storage/EZ2AYPMW/Ekici and Jawzal - 2020 - Breast cancer diagnosis using thermography and con.pdf;/Users/isalykkehansen/Zotero/storage/RE96NYXU/S0306987719313659.html},
  journal = {Medical Hypotheses},
  keywords = {Breast cancer,Breast thermal image,Convolutional neural network,Image analysis,Thermography},
  language = {en}
}

@misc{FramebyframeNewApproach,
  title = {Frame-by-Frame: A New Approach for Monitoring Plant-Pollinator Interactions by Time Lapse Photography - Project - {{Framsenteret AS}}},
  file = {/Users/isalykkehansen/Zotero/storage/7ISSG9QL/db.343156.no.html},
  howpublished = {http://www.ifram.no/db.343156.no.html?lid=561.23b3c679cc0a7fb3c3feae33075e126b}
}

@article{hansenSpecieslevelImageClassification2020,
  title = {Species-Level Image Classification with Convolutional Neural Network Enables Insect Identification from Habitus Images},
  author = {Hansen, Oskar L. P. and Svenning, Jens-Christian and Olsen, Kent and Dupont, Steen and Garner, Beulah H. and Iosifidis, Alexandros and Price, Benjamin W. and H\o ye, Toke T.},
  year = {2020},
  volume = {10},
  pages = {737--747},
  issn = {2045-7758},
  doi = {10.1002/ece3.5921},
  abstract = {Changes in insect biomass, abundance, and diversity are challenging to track at sufficient spatial, temporal, and taxonomic resolution. Camera traps can capture habitus images of ground-dwelling insects. However, currently sampling involves manually detecting and identifying specimens. Here, we test whether a convolutional neural network (CNN) can classify habitus images of ground beetles to species level, and estimate how correct classification relates to body size, number of species inside genera, and species identity. We created an image database of 65,841 museum specimens comprising 361 carabid beetle species from the British Isles and fine-tuned the parameters of a pretrained CNN from a training dataset. By summing up class confidence values within genus, tribe, and subfamily and setting a confidence threshold, we trade-off between classification accuracy, precision, and recall and taxonomic resolution. The CNN classified 51.9\% of 19,164 test images correctly to species level and 74.9\% to genus level. Average classification recall on species level was 50.7\%. Applying a threshold of 0.5 increased the average classification recall to 74.6\% at the expense of taxonomic resolution. Higher top value from the output layer and larger sized species were more often classified correctly, as were images of species in genera with few species. Fine-tuning enabled us to classify images with a high mean recall for the whole test dataset to species or higher taxonomic levels, however, with high variability. This indicates that some species are more difficult to identify because of properties such as their body size or the number of related species. Together, species-level image classification of arthropods from museum collections and ecological monitoring can substantially increase the amount of occurrence data that can feasibly be collected. These tools thus provide new opportunities in understanding and predicting ecological responses to environmental change.},
  copyright = {\textcopyright{} 2019 The Authors. Ecology and Evolution published by John Wiley \& Sons Ltd.},
  file = {/Users/isalykkehansen/Zotero/storage/IJMNDHR7/Hansen et al. - 2020 - Species-level image classification with convolutio.pdf;/Users/isalykkehansen/Zotero/storage/MWPIQGYU/ece3.html},
  journal = {Ecology and Evolution},
  keywords = {arthropod sampling,automatic species identification,camera trap,entomological collection,image classification,image database},
  language = {en},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.5921},
  number = {2}
}

@article{krastevRealtimeDetectionGravitational2020,
  title = {Real-Time Detection of Gravitational Waves from Binary Neutron Stars Using Artificial Neural Networks},
  author = {Krastev, Plamen G.},
  year = {2020},
  month = apr,
  volume = {803},
  pages = {135330},
  issn = {0370-2693},
  doi = {10.1016/j.physletb.2020.135330},
  abstract = {The groundbreaking discoveries of gravitational waves from binary black-hole mergers [1], [2], [3] and, most recently, coalescing neutron stars [4] started a new era of Multi-Messenger Astrophysics and revolutionized our understanding of the Cosmos. Machine learning techniques such as artificial neural networks are already transforming many technological fields and have also proven successful in gravitational-wave astrophysics for detection and characterization of gravitational-wave signals from binary black holes [5], [6], [7]. Here we use a deep-learning approach to rapidly identify transient gravitational-wave signals from binary neutron star mergers in noisy time series representative of typical gravitational-wave detector data. Specifically, we show that a deep convolution neural network trained on 100,000 data samples can promptly identify binary neutron star gravitational-wave signals and distinguish them from noise and signals from merging black hole binaries. These results demonstrate the potential of artificial neural networks for real-time detection of gravitational-wave signals from binary neutron star mergers, which is critical for a prompt follow-up and detailed observation of the electromagnetic and astro-particle counterparts accompanying these important transients.},
  file = {/Users/isalykkehansen/Zotero/storage/FWB98NJL/Krastev - 2020 - Real-time detection of gravitational waves from bi.pdf;/Users/isalykkehansen/Zotero/storage/3HB8448C/S0370269320301349.html},
  journal = {Physics Letters B},
  language = {en}
}

@misc{MachineLearningWhen,
  title = {Machine Learning - {{When}} Should {{I}} Balance Classes in a Training Data Set?},
  file = {/Users/isalykkehansen/Zotero/storage/K8TNYT7W/when-should-i-balance-classes-in-a-training-data-set.html},
  howpublished = {https://stats.stackexchange.com/questions/227088/when-should-i-balance-classes-in-a-training-data-set},
  journal = {Cross Validated}
}

@misc{NotAllFlowers,
  title = {Not All Flowers Are Equal: {{Plant}}-Pollinator Interactions in a Changing {{Arctic}} \textendash{} {{Unisprout}}},
  shorttitle = {Not All Flowers Are Equal},
  file = {/Users/isalykkehansen/Zotero/storage/LYUZQPS6/not-all-flowers-are-equal-plant-pollinator-interactions-in-a-changing-arctic.html},
  language = {en-US}
}

@misc{olsenCvms2020,
  title = {Cvms},
  author = {Olsen, Ludvig Renbo},
  year = {2020}
}

@misc{olsenGroupdata22020,
  title = {Groupdata2},
  author = {Olsen, Ludvig Renbo},
  year = {2020}
}

@misc{pythonsoftwarefoundationPython2020,
  title = {Python},
  author = {Python Software Foundation},
  year = {2020}
}

@misc{radecicTakingDerivativesPython2019,
  title = {Taking {{Derivatives}} in {{Python}}},
  author = {Rade{\v c}i{\'c}, Dario},
  year = {2019},
  month = oct,
  abstract = {Learn how to deal with Calculus part of Machine Learning},
  file = {/Users/isalykkehansen/Zotero/storage/6CG5AVXF/taking-derivatives-in-python-d6229ba72c64.html},
  howpublished = {https://towardsdatascience.com/taking-derivatives-in-python-d6229ba72c64},
  journal = {Medium},
  language = {en}
}

@article{redmonYOLOv3IncrementalImprovement,
  title = {{{YOLOv3}}: {{An Incremental Improvement}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  pages = {6},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320 \texttimes{} 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP50 in 51 ms on a Titan X, compared to 57.5 AP50 in 198 ms by RetinaNet, similar performance but 3.8\texttimes{} faster. As always, all the code is online at https://pjreddie.com/yolo/.},
  file = {/Users/isalykkehansen/Zotero/storage/UKDWJTV8/Redmon and Farhadi - YOLOv3 An Incremental Improvement.pdf},
  language = {en}
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = jun,
  pages = {779--788},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.91},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  file = {/Users/isalykkehansen/Zotero/storage/H5NK9UUR/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf},
  isbn = {978-1-4673-8851-1},
  language = {en}
}

@misc{vestfalenAarhusHeleByen2019,
  title = {Aarhus: {{Hele}} Byen Er Digitalt Testcenter},
  shorttitle = {Aarhus},
  author = {Vestfalen, Jonas},
  year = {2019},
  month = nov,
  abstract = {Aarhus Kommune har sat Smart City h\o jt p\aa{} dagsordenen. Med digitalt epicenter fra DOKK1 har Bo Fristed, chef for ITK, hele kommunen som digitalt testcenter med et menukort, der byder p\aa{} TAPAS, City Sharks og Octocopters.},
  file = {/Users/isalykkehansen/Zotero/storage/9YR3ENCK/aarhus-hele-byen-er-digitalt-testcenter.html},
  howpublished = {https://www.coi.dk/nyheder/2019/november/aarhus-hele-byen-er-digitalt-testcenter/},
  language = {en}
}

@article{virtanenSciPyFundamentalAlgorithms2020,
  title = {{{SciPy}} 1.0: Fundamental Algorithms for Scientific Computing in {{Python}}},
  shorttitle = {{{SciPy}} 1.0},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul},
  year = {2020},
  month = mar,
  volume = {17},
  pages = {261--272},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0686-2},
  abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
  copyright = {2020 The Author(s)},
  file = {/Users/isalykkehansen/Zotero/storage/RRJU94PN/Virtanen et al. - 2020 - SciPy 1.0 fundamental algorithms for scientific c.pdf;/Users/isalykkehansen/Zotero/storage/EKY8G3DX/s41592-019-0686-2.html},
  journal = {Nature Methods},
  language = {en},
  number = {3}
}

@inproceedings{yuImageComplexitySpatial2013,
  title = {Image Complexity and Spatial Information},
  booktitle = {2013 {{Fifth International Workshop}} on {{Quality}} of {{Multimedia Experience}} ({{QoMEX}})},
  author = {Yu, Honghai and Winkler, Stefan},
  year = {2013},
  month = jul,
  pages = {12--17},
  publisher = {{IEEE}},
  address = {{Klagenfurt am W\"orthersee, Austria}},
  doi = {10.1109/QoMEX.2013.6603194},
  abstract = {The complexity of an image tells many aspects of the image content and is an important factor in the selection of source material for testing various image processing methods. We explore objective measures of complexity that are based on compression. We show that spatial information (SI) measures strongly correlate with compression-based complexity measures. Among the commonly used SI measures, the mean of the edge magnitude is shown to be the best predictor. Moreover, we find that compression-based complexity of an image normally increases with decreasing resolution.},
  file = {/Users/isalykkehansen/Zotero/storage/QFUMKKAC/Yu and Winkler - 2013 - Image complexity and spatial information.pdf},
  isbn = {978-1-4799-0738-0},
  language = {en}
}

@article{zanetteQuantifyingComplexityBlackandwhite2018,
  title = {Quantifying the Complexity of Black-and-White Images},
  author = {Zanette, Dami{\'a}n H.},
  editor = {Chialvo, Dante R.},
  year = {2018},
  month = nov,
  volume = {13},
  pages = {e0207879},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0207879},
  abstract = {We propose a complexity measure for black-and-white (B/W) digital images, based on the detection of typical length scales in the depicted motifs. Complexity is associated with diversity in those length scales. In this sense, the proposed measure penalizes images where typical scales are limited to small lengths, of a few pixels \textendash as in an image where gray levels are distributed at random\textendash{} or to lengths similar to the image size \textendash as when gray levels are ordered into a simple, broad pattern. We introduce a complexity index which captures the structural richness of images with a wide range of typical scales, and compare several images with each other on the basis of this index. Since the index provides an objective quantification of image complexity, it could be used as the counterpart of subjective visual complexity in experimental perception research. As an application of the complexity index, we build a ``complexity map'' for South-American topography, by analyzing a large B/W image that represents terrain elevation data in the continent. Results show that the complexity index is able to clearly reveal regions with intricate topographical features such as river drainage networks and fjord-like coasts. Although, for the sake of concreteness, our complexity measure is introduced for B/W images, the definition can be straightforwardly extended to any object that admits a mathematical representation as a function of one or more variables. Thus, the quantification of structural richness can be adapted to time signals and distributions of various kinds.},
  file = {/Users/isalykkehansen/Zotero/storage/JQJGEF3C/Zanette - 2018 - Quantifying the complexity of black-and-white imag.pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {11}
}


